{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Understanding\n",
    "\n",
    "- Neural networks are machine learning models that are designed to mimic the structure and function of the human brain. \n",
    "\n",
    "- They are composed of interconnected nodes (neurons) that are organized into layers. \n",
    "\n",
    "**Input layer**: receives the raw data that is being analyzed\n",
    "\n",
    "**Hidden layer(s)**: performs calculations on the input data and generates an output\n",
    "\n",
    "**Output layer**: produces the final result of the analysis\n",
    "\n",
    "The strength of the connections between the neurons (synapses) is adjusted during the training process, in order to optimize the performance of the network on a particular task.\n",
    "\n",
    "### Why are neural networks useful in data science?\n",
    "- Neural networks are capable of learning complex patterns in large datasets, without being explicitly programmed for each task.\n",
    "\n",
    "**Image recognition**: identifying objects in images or video\n",
    "\n",
    "**Natural language processing: understanding and generating human language\n",
    "\n",
    "**Predictive modeling**: forecasting future trends or events based on historical data\n",
    "\n",
    "By using neural networks, data scientists can automate the process of extracting insights from data, making it more efficient and accurate.\n",
    "\n",
    "### How are neural networks applied?\n",
    "Neural networks can be applied to data science problems in a few different ways:\n",
    "\n",
    "**Supervised learning**: the network is trained on labeled data (where the correct output is known) to learn to predict new outputs for similar inputs\n",
    "\n",
    "**Unsupervised learning**: the network is trained on unlabeled data to discover underlying patterns or structures in the data\n",
    "\n",
    "**Reinforcement learning**: the network is trained to make decisions based on feedback from its environment, with the goal of maximizing a reward function\n",
    "\n",
    "Some common neural network architectures include:\n",
    "\n",
    "**Convolutional neural networks (CNNs)**: used for image recognition and computer vision tasks\n",
    "\n",
    "**Recurrent neural networks (RNNs)**: used for natural language processing and time series analysis\n",
    "\n",
    "**Feedforward neural networks (FNNs)**: used for predictive modeling and classification tasks\n",
    "\n",
    "- Neural networks allow for complex data analysis and pattern recognition without the need for explicit programming. \n",
    "- By training neural networks on large datasets, data scientists can improve the accuracy and efficiency of their analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction to Neural Networks\n",
    "\n",
    "## 2.1 Background\n",
    "Neural networks have been around for a while. They are over 70 years old, dating back to  their proposal in 1944 by Warren McCullough and Walter Pitts. These first proposed neural nets had thresholds and weights, but no layers and no specific training mechanisms.\n",
    "\n",
    "The \"perceptron\", the first trainable neural network, was created by Frank Rosenblatt in 1957. It consisted of a single layer with adjustable weights in the middle of input and output layers.\n",
    "\n",
    "![perceptron](images/nn-diagram.png)\n",
    "\n",
    "## 2.2 Wait, Wait, Wait... Why a Neural Network?\n",
    "\"Do we really need to use this 'neural network' if we already have so many machine learning algorithms?\"\n",
    "\n",
    "And in short, we don't need to default to a neural network but they have advantages in solving very complex problems. It might help to know that idea of neural networks was developed back in the 1950s (perceptron network). It wasn't until we had a lot of data and computational power where they became reasonably useful.\n",
    "\n",
    "## 2.3 Starting with a Perceptron\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*No3vRruq7Dd4sxdn.png' width=40%/>\n",
    "\n",
    "A Perceptron is the simplest type of neural network, consisting of a single layer of neurons with weighted connections to the input variables and a single output. It can be used for binary classification problems, where the output is either 0 or 1.\n",
    "\n",
    "## 2.4 Relation to Previous Models\n",
    "\n",
    "### 2.4.1 Logistic Regression\n",
    "Think of the weights as the betas and the activation function as the sigmoid function!\n",
    "\n",
    "### 2.4.2 Stacking Ensembles\n",
    "Various base models' predictions are fed into a \"meta-estimator\" that is trained to aggregate them optimally. This is analogous to the multiple **layers** of a neural network.\n",
    "\n",
    "## 2.5 Basic Architecture\n",
    "For our DS purposes, we'll generally imagine our network to consist of only a few layers, including an input layer (where we feed in our data) an output layer (comprising our predictions). Significantly, there will also (generally) be one or more layers of neurons between input and output, called **hidden layers**.\n",
    "\n",
    "One reason these are named hidden layers is that what their output actually represents in not really known.  The activation of node 1 of the first hidden layer may represent a sequence of pixel intensity corresponding to a horizontal line, or a group of dark pixels in the middle of a number's loop. \n",
    "\n",
    "![dense](images/Deeper_network.jpg)\n",
    "\n",
    "Because we are unaware of how exactly these hidden layers are operating, neural networks are considered **black box** algorithms.  You will not be able to gain much inferential insight from a neural net.\n",
    "\n",
    "Each of our pixels from our digit representation goes to each of our nodes, and each node has a set of weights and a bias term associated with it.\n",
    "\n",
    "## 2.6 Inspiration from Actual Neurons\n",
    "The composition of neural networks can be **loosely** compared to a neuron.\n",
    "\n",
    "![neuron](images/neuron.png)\n",
    "\n",
    "Neural networks draw their inspiration from the biology of our own brains, which are of course also accurately described as 'neural networks'. A human brain contains around $10^{11}$ neurons, connected very **densely**.\n",
    "\n",
    "This is a loose analogy, but can be a helpful **mnemonic**. The inputs to our node are like inputs to our neurons. They are either direct sensory information (our features) or input from other axons (nodes passing information to other nodes). The body of our neuron (soma) is where the signals of the dendrites are summed together, which is loosely analogous to our **collector function**. If the summed signal is large enough (our **activation function**), they trigger an action potential which travels down the axon to be passed as output to other dendrites. See [here](https://en.wikipedia.org/wiki/Neuron) for more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Networks Overview\n",
    "\n",
    "## Couple ways to think of neural networks\n",
    "\n",
    "> We can think of neural networks as finding ways to take inputs and creating something like latent features.\n",
    "\n",
    "![](images/neural_network_with_human_readable_labels.png)\n",
    "\n",
    "> But we can also think of them as creating linear separators and then combining them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2554679e280>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x = np.random.rand(25)\n",
    "y = np.random.rand(25)\n",
    "z = (x + y) <= 0.8\n",
    "plt.scatter(x, y, c=z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/neural_network_mathematics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of a Neural Network\n",
    "A neural network consists of several basic parts that work together to process data inputs and produce an output. These parts include:\n",
    "\n",
    "## Layers\n",
    "- **Input Layer**: the initial parameters (these will be the parts we feed to our network)\n",
    "- **Output Layer**: the classification (or regression predictions)\n",
    "- **Hidden Layer(s)**: the other neurons potentially in a neural network to find more complex patterns\n",
    "\n",
    "## Weights\n",
    "\n",
    "> The weights from our inputs are describing how much they should contribute to the next neuron\n",
    "\n",
    "But we can also think of the weights of hidden layer neurons telling us how much of these linear separations should be combined.\n",
    "\n",
    "## Activation Functions\n",
    "![activation](images/log-reg-nn-ex-a.png)\n",
    "\n",
    "- Then we pass it into an activation function. The activation function converts our summed inputs into an output, which is then passed on to other nodes in hidden layers, or as an end product in the output layer. This can loosely be thought of as the action potential traveling down the axon.\n",
    "\n",
    "- When we build our models in `keras`, we will specify the activation function of both hidden layers and output.\n",
    "\n",
    "## Other Hyperparameters\n",
    "- **Learning Rate ($\\alpha$)**: how big of a step we take in gradient descent\n",
    "- **Number of Epochs**: how many times we repeat this process\n",
    "- **Batch Size**: how many data points we use in a single training session (1 epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. How Neural Networks Work\n",
    "Neural networks are a type of machine learning algorithm modeled after the structure of the human brain. They are used to recognize patterns and make predictions based on that data.\n",
    "\n",
    "## 4.1 Forward Propagation\n",
    "Forward propagation is the process of moving input data through a neural network to produce an output. This involves several steps:\n",
    "\n",
    "## 4.1.1 Summation\n",
    "The inputs are multiplied by weights and then all the products are summed up to produce a single value. This process is repeated for every neuron in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7\n"
     ]
    }
   ],
   "source": [
    "# Example code for summation\n",
    "inputs = [1, 2, 3, 4]\n",
    "weights = [0.5, 0.3, -0.2, 0.8]\n",
    "output = sum([i * w for i, w in zip(inputs, weights)])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Activation Functions\n",
    "The result of the summation is then passed through an activation function, which produces the output of the neuron. Activation functions introduce non-linearity into the neural network, allowing it to learn more complex patterns.\n",
    "\n",
    "## 4.1.2.1 Sigmoid\n",
    "The sigmoid function is commonly used as an activation function. It maps any input to a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6224593312018546\n"
     ]
    }
   ],
   "source": [
    "# Example code for sigmoid function\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "output = sigmoid(0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2.2 tanh\n",
    "The tanh function is another activation function that maps any input to a value between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46211715726000974\n"
     ]
    }
   ],
   "source": [
    "# Example code for tanh function\n",
    "import math\n",
    "\n",
    "def tanh(x):\n",
    "    return math.tanh(x)\n",
    "\n",
    "output = tanh(0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2.3 ReLU\n",
    "The ReLU (Rectified Linear Unit) function is a popular activation function that returns the input if it is positive, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Example code for ReLU function\n",
    "def ReLU(x):\n",
    "    return max(0, x)\n",
    "\n",
    "output = ReLU(0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2.4 Swish\n",
    "The Swish function is a relatively new activation function that was proposed in 2017. It is similar to the sigmoid function but allows for negative input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3112296656009273\n"
     ]
    }
   ],
   "source": [
    "# Example code for Swish function\n",
    "import math\n",
    "\n",
    "def Swish(x):\n",
    "    return x / (1 + math.exp(-x))\n",
    "\n",
    "output = Swish(0.5)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2.5 Softmax\n",
    "The softmax function is often used as the activation function for the output layer in a neural network. It maps any input to a probability distribution over several categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27283828 0.22338109 0.13548748 0.36829315]\n"
     ]
    }
   ],
   "source": [
    "# Example code for softmax function\n",
    "import numpy as np\n",
    "\n",
    "def Softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "output = Softmax([0.5, 0.3, -0.2, 0.8])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Keras and Tensorflow\n",
    "\n",
    "\n",
    "## backpropagation:\n",
    "Backpropagation is the algorithm used to train artificial neural networks by propagating the errors backward from the output layer to the input layer. During training, the algorithm adjusts the weights of each neuron in the network in proportion to its contribution to the overall error. This iterative process continues until the error is minimized.\n",
    "\n",
    "## The use of gradient descent in neural networks:\n",
    "Gradient descent is an optimization algorithm that is used to minimize the cost function of a neural network. During training, the algorithm calculates the gradient of the cost function with respect to the weights of the network and updates the weights in the opposite direction of the gradient to minimize the error.\n",
    "\n",
    "## Keras API and its role in backend:\n",
    "Keras is a high-level neural network API that is used to build and train deep learning models. It provides a user-friendly interface for building neural networks, making it easier for developers to create complex models without having to worry about the underlying implementation details. Keras can be used with a variety of backends, including TensorFlow, Theano, and Microsoft Cognitive Toolkit.\n",
    "\n",
    "## TensorFlow and its role in neural network modeling:\n",
    "TensorFlow is an open-source software library used for numerical computation and large-scale machine learning. It provides a flexible and efficient platform for building and deploying machine learning models, including neural networks. TensorFlow is widely used in both academia and industry and has become one of the most popular deep learning frameworks available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial: Using TensorFlow to build a binary classifier neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sequential** is a class that allows us to create a model layer-by-layer.\n",
    "\n",
    "- **Dense** is a class that creates fully connected layers.\n",
    "\n",
    "We can define the network and its layers, compile the model, and fit it to our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this example, we define a model with three layers: \n",
    "        - The input layer with 8 nodes, a hidden layer with 12 nodes, and another hidden layer with 8 nodes. \n",
    "        - The output layer has one node with a sigmoid activation function, which is appropriate for binary classification problems.\n",
    "- We compile the model using binary cross-entropy as the loss function, Adam as the optimizer, and accuracy as the metric to evaluate the model.\n",
    "- Finally, we fit the model using our training data, specifying the number of epochs and batch size.\n",
    "\n",
    "After fitting the model, we can evaluate its performance on our test data using the evaluate method of the model object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
